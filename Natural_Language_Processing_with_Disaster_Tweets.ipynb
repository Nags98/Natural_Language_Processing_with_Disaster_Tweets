{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CODE:","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:27.469352Z","iopub.execute_input":"2023-10-30T12:16:27.469736Z","iopub.status.idle":"2023-10-30T12:16:27.919162Z","shell.execute_reply.started":"2023-10-30T12:16:27.469705Z","shell.execute_reply":"2023-10-30T12:16:27.918144Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# ! pip install contractions\n# ! pip install pyspellchecker","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:27.921330Z","iopub.execute_input":"2023-10-30T12:16:27.921933Z","iopub.status.idle":"2023-10-30T12:16:27.925719Z","shell.execute_reply.started":"2023-10-30T12:16:27.921895Z","shell.execute_reply":"2023-10-30T12:16:27.924838Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport cv2\nimport timm\nimport torch\nimport string\nimport numpy as np \nimport pandas as pd\n# import contractions\nfrom tqdm import tqdm\nimport torch.nn as nn\nfrom torch import tensor\nimport torch.nn.functional as F\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torchvision\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import f1_score\n# from spellchecker import SpellChecker\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset as BaseDataset, DataLoader, TensorDataset, RandomSampler\nfrom transformers import BertTokenizer, DistilBertTokenizer,RobertaTokenizer,AlbertTokenizer,XLNetTokenizer, BertModel, BertForSequenceClassification, DistilBertForSequenceClassification, RobertaForSequenceClassification, AlbertForSequenceClassification, XLNetForSequenceClassification","metadata":{"papermill":{"duration":14.582025,"end_time":"2023-10-06T06:05:27.143196","exception":false,"start_time":"2023-10-06T06:05:12.561171","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-30T12:16:27.926922Z","iopub.execute_input":"2023-10-30T12:16:27.927563Z","iopub.status.idle":"2023-10-30T12:16:43.824925Z","shell.execute_reply.started":"2023-10-30T12:16:27.927531Z","shell.execute_reply":"2023-10-30T12:16:43.824117Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":0.014731,"end_time":"2023-10-06T06:05:27.472430","exception":false,"start_time":"2023-10-06T06:05:27.457699","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-30T12:16:43.826998Z","iopub.execute_input":"2023-10-30T12:16:43.827677Z","iopub.status.idle":"2023-10-30T12:16:43.832141Z","shell.execute_reply.started":"2023-10-30T12:16:43.827647Z","shell.execute_reply":"2023-10-30T12:16:43.831135Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=2):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\nset_seed(99)","metadata":{"papermill":{"duration":0.019533,"end_time":"2023-10-06T06:05:30.122993","exception":false,"start_time":"2023-10-06T06:05:30.103460","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-30T12:16:43.833330Z","iopub.execute_input":"2023-10-30T12:16:43.833568Z","iopub.status.idle":"2023-10-30T12:16:43.847788Z","shell.execute_reply.started":"2023-10-30T12:16:43.833547Z","shell.execute_reply":"2023-10-30T12:16:43.846864Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"papermill":{"duration":0.09348,"end_time":"2023-10-06T06:05:30.224336","exception":false,"start_time":"2023-10-06T06:05:30.130856","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-30T12:16:43.849209Z","iopub.execute_input":"2023-10-30T12:16:43.849536Z","iopub.status.idle":"2023-10-30T12:16:43.883213Z","shell.execute_reply.started":"2023-10-30T12:16:43.849504Z","shell.execute_reply":"2023-10-30T12:16:43.882289Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"papermill":{"duration":0.371601,"end_time":"2023-10-06T06:05:30.604125","exception":false,"start_time":"2023-10-06T06:05:30.232524","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-30T12:16:43.884300Z","iopub.execute_input":"2023-10-30T12:16:43.884644Z","iopub.status.idle":"2023-10-30T12:16:43.960275Z","shell.execute_reply.started":"2023-10-30T12:16:43.884597Z","shell.execute_reply":"2023-10-30T12:16:43.959477Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:43.961341Z","iopub.execute_input":"2023-10-30T12:16:43.961627Z","iopub.status.idle":"2023-10-30T12:16:43.975866Z","shell.execute_reply.started":"2023-10-30T12:16:43.961596Z","shell.execute_reply":"2023-10-30T12:16:43.975175Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"papermill":{"duration":0.031954,"end_time":"2023-10-06T06:05:30.667279","exception":false,"start_time":"2023-10-06T06:05:30.635325","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-30T12:16:43.976870Z","iopub.execute_input":"2023-10-30T12:16:43.977150Z","iopub.status.idle":"2023-10-30T12:16:43.994950Z","shell.execute_reply.started":"2023-10-30T12:16:43.977127Z","shell.execute_reply":"2023-10-30T12:16:43.994090Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:43.999815Z","iopub.execute_input":"2023-10-30T12:16:44.000093Z","iopub.status.idle":"2023-10-30T12:16:44.009911Z","shell.execute_reply.started":"2023-10-30T12:16:44.000051Z","shell.execute_reply":"2023-10-30T12:16:44.008911Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def clean_dataset(text):\n    text = text.lower()\n    url_pattern = re.compile(r'http\\S+')\n    cleaned_text = url_pattern.sub('', text)\n    text = ' '.join(cleaned_text.split())\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '',text) #Removes Websites\n    text  = re.sub(r'<.*?>' ,'', text) \n    text = re.sub(r'\\x89\\S+' , ' ', text) #Removes string starting from \\x89\n    text = re.sub('\\w*\\d\\w*', '', text)  # Removes numbers\n    table = str.maketrans('','',string.punctuation)\n    text = text.translate(table)\n    text = re.sub(r'[^\\w\\s]','',text)   # Removes Punctuations\n#     emoji_pattern = re.compile(\"[\"\n#                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n#                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n#                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n#                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n#                                u\"\\U00002500-\\U00002BEF\"  # chinese char\n#                                u\"\\U00002702-\\U000027B0\"\n#                                u\"\\U00002702-\\U000027B0\"\n#                                u\"\\U000024C2-\\U0001F251\"\n#                                u\"\\U0001f926-\\U0001f937\"\n#                                u\"\\U00010000-\\U0010ffff\"\n#                                u\"\\u2640-\\u2642\"\n#                                u\"\\u2600-\\u2B55\"\n#                                u\"\\u200d\"\n#                                u\"\\u23cf\"\n#                                u\"\\u23e9\"\n#                                u\"\\u231a\"\n#                                u\"\\ufe0f\"  # dingbats\n#                                u\"\\u3030\"\n#                                \"]+\", flags=re.UNICODE)\n#     text = emoji_pattern.sub(r'', text)\n    text = contractions.fix(text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'\\s+([A-Z])', r'\\1', text)\n    text = correct_spellings(text)\n    return text\n\ndef correct_spellings(text):\n    spell = SpellChecker()\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(str(spell.correction(word)))\n        else:\n            corrected_text.append(str(word))\n    return \" \".join(corrected_text)\n\ndef Convert(string):\n    li = list(string.lower().split(\" \"))\n    return li","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.011524Z","iopub.execute_input":"2023-10-30T12:16:44.011911Z","iopub.status.idle":"2023-10-30T12:16:44.024225Z","shell.execute_reply.started":"2023-10-30T12:16:44.011881Z","shell.execute_reply":"2023-10-30T12:16:44.023384Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# train_df[\"text_clean\"] = train_df[\"text\"].map(clean_dataset)\n# train_df[\"List of Words\"] = train_df[\"text_clean\"].map(Convert)\n# test_df[\"text_clean\"] = test_df[\"text\"].map(clean_dataset)\n# test_df[\"List of Words\"] = test_df[\"text_clean\"].map(Convert)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.025292Z","iopub.execute_input":"2023-10-30T12:16:44.025636Z","iopub.status.idle":"2023-10-30T12:16:44.039136Z","shell.execute_reply.started":"2023-10-30T12:16:44.025585Z","shell.execute_reply":"2023-10-30T12:16:44.038283Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.040033Z","iopub.execute_input":"2023-10-30T12:16:44.040311Z","iopub.status.idle":"2023-10-30T12:16:44.056253Z","shell.execute_reply.started":"2023-10-30T12:16:44.040289Z","shell.execute_reply":"2023-10-30T12:16:44.055338Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.057475Z","iopub.execute_input":"2023-10-30T12:16:44.057743Z","iopub.status.idle":"2023-10-30T12:16:44.072566Z","shell.execute_reply.started":"2023-10-30T12:16:44.057721Z","shell.execute_reply":"2023-10-30T12:16:44.071503Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df['text'].values[5]","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.073792Z","iopub.execute_input":"2023-10-30T12:16:44.074092Z","iopub.status.idle":"2023-10-30T12:16:44.085807Z","shell.execute_reply.started":"2023-10-30T12:16:44.074047Z","shell.execute_reply":"2023-10-30T12:16:44.084988Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'"},"metadata":{}}]},{"cell_type":"code","source":"# train_df['text_clean'].values[5]","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.086900Z","iopub.execute_input":"2023-10-30T12:16:44.087188Z","iopub.status.idle":"2023-10-30T12:16:44.096992Z","shell.execute_reply.started":"2023-10-30T12:16:44.087165Z","shell.execute_reply":"2023-10-30T12:16:44.096060Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_csv, other = train_test_split(train_df, test_size = 0.2, stratify = train_df['target'], random_state = 42)\nvalid_csv, test_csv = train_test_split(other, test_size = 0.3, stratify = other['target'], random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.098058Z","iopub.execute_input":"2023-10-30T12:16:44.098389Z","iopub.status.idle":"2023-10-30T12:16:44.121143Z","shell.execute_reply.started":"2023-10-30T12:16:44.098365Z","shell.execute_reply":"2023-10-30T12:16:44.120129Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_csv.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.122306Z","iopub.execute_input":"2023-10-30T12:16:44.122543Z","iopub.status.idle":"2023-10-30T12:16:44.133179Z","shell.execute_reply.started":"2023-10-30T12:16:44.122522Z","shell.execute_reply":"2023-10-30T12:16:44.132227Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"         id         keyword                       location  \\\n6234   8902       snowstorm                     South, USA   \n326     472      armageddon                      Worldwide   \n997    1448  body%20bagging                        Cloud 9   \n7269  10407       whirlwind  Sheff/Bangor/Salamanca/Madrid   \n2189   3137          debris                       Nigeria    \n\n                                                   text  target  \n6234  Sassy city girl country hunk stranded in Smoky...       1  \n326   God's Kingdom (Heavenly Gov't) will rule over ...       0  \n997   Mopheme and Bigstar Johnson are a problem in t...       0  \n7269          @VixMeldrew sounds like a whirlwind life!       0  \n2189  Malaysia confirms plane debris washed up on Re...       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6234</th>\n      <td>8902</td>\n      <td>snowstorm</td>\n      <td>South, USA</td>\n      <td>Sassy city girl country hunk stranded in Smoky...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>326</th>\n      <td>472</td>\n      <td>armageddon</td>\n      <td>Worldwide</td>\n      <td>God's Kingdom (Heavenly Gov't) will rule over ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>1448</td>\n      <td>body%20bagging</td>\n      <td>Cloud 9</td>\n      <td>Mopheme and Bigstar Johnson are a problem in t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7269</th>\n      <td>10407</td>\n      <td>whirlwind</td>\n      <td>Sheff/Bangor/Salamanca/Madrid</td>\n      <td>@VixMeldrew sounds like a whirlwind life!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2189</th>\n      <td>3137</td>\n      <td>debris</td>\n      <td>Nigeria</td>\n      <td>Malaysia confirms plane debris washed up on Re...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizer, DistilBertTokenizer, RobertaTokenizer, AlbertTokenizer, XLNetTokenizer, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.134296Z","iopub.execute_input":"2023-10-30T12:16:44.134586Z","iopub.status.idle":"2023-10-30T12:16:44.146061Z","shell.execute_reply.started":"2023-10-30T12:16:44.134563Z","shell.execute_reply":"2023-10-30T12:16:44.145326Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:44.147446Z","iopub.execute_input":"2023-10-30T12:16:44.147711Z","iopub.status.idle":"2023-10-30T12:16:45.776532Z","shell.execute_reply.started":"2023-10-30T12:16:44.147688Z","shell.execute_reply":"2023-10-30T12:16:45.775503Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fef26f900104e8abd1806f96f3a5372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"253fa5f8bc40461389cb79c05490e581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7f1c490bea4ff58eee44cd963868b6"}},"metadata":{}}]},{"cell_type":"code","source":"tok=[]\nfor index, row in train_df.iterrows():\n    text = row['text']\n    tokens = list(set(tokenizer.encode(text, add_special_tokens=False)))\n    tok.extend(tokens)\n\nprint(f\"Total tokens in the dataset: {len(list(set(tok)))}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:45.777812Z","iopub.execute_input":"2023-10-30T12:16:45.778177Z","iopub.status.idle":"2023-10-30T12:16:50.350311Z","shell.execute_reply.started":"2023-10-30T12:16:45.778143Z","shell.execute_reply":"2023-10-30T12:16:50.349309Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Total tokens in the dataset: 19405\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize(data,max_len=512) :\n    encoded = tokenizer.batch_encode_plus(\n                                            list(data['text'].values),\n                                            add_special_tokens=True,\n                                            return_attention_mask=True,\n                                            pad_to_max_length=True,\n                                            max_length=256,\n                                            return_tensors='pt'\n                                        )\n    return encoded['input_ids'], encoded['attention_mask'], (torch.tensor(data.target.values))\n\ndef pred_tokenize(data,max_len=512) :\n    encoded = tokenizer.batch_encode_plus(\n                                            list(data['text'].values),\n                                            add_special_tokens=True,\n                                            return_attention_mask=True,\n                                            pad_to_max_length=True,\n                                            max_length=256,\n                                            return_tensors='pt'\n                                        )\n    return encoded['input_ids'], encoded['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:50.351579Z","iopub.execute_input":"2023-10-30T12:16:50.351885Z","iopub.status.idle":"2023-10-30T12:16:50.358727Z","shell.execute_reply.started":"2023-10-30T12:16:50.351858Z","shell.execute_reply":"2023-10-30T12:16:50.357820Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_csv.loc[0]['text']","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:50.359957Z","iopub.execute_input":"2023-10-30T12:16:50.360318Z","iopub.status.idle":"2023-10-30T12:16:50.381331Z","shell.execute_reply.started":"2023-10-30T12:16:50.360286Z","shell.execute_reply":"2023-10-30T12:16:50.380419Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"},"metadata":{}}]},{"cell_type":"code","source":"input_ids_train, attention_masks_train, labels_train = tokenize(train_csv)\ninput_ids_test, attention_masks_test, labels_test = tokenize(test_csv)\ninput_ids_valid, attention_masks_valid, labels_valid = tokenize(valid_csv)\ninput_ids_pred, attention_masks_pred = pred_tokenize(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:50.382432Z","iopub.execute_input":"2023-10-30T12:16:50.382678Z","iopub.status.idle":"2023-10-30T12:16:55.857705Z","shell.execute_reply.started":"2023-10-30T12:16:50.382657Z","shell.execute_reply":"2023-10-30T12:16:55.856876Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train, attention_masks_train,labels_train)#, sampler = RandomSampler(train_dataset))\ndataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)#, sampler = RandomSampler(test_dataset))\ndataset_val = TensorDataset(input_ids_valid, attention_masks_valid, labels_valid)#, sampler = RandomSampler(val_dataset))\ndataset_pred = TensorDataset(input_ids_pred, attention_masks_pred)#, sampler = RandomSampler(val_dataset))","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.858895Z","iopub.execute_input":"2023-10-30T12:16:55.859204Z","iopub.status.idle":"2023-10-30T12:16:55.864675Z","shell.execute_reply.started":"2023-10-30T12:16:55.859178Z","shell.execute_reply":"2023-10-30T12:16:55.863528Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset_train,batch_size=4)\ntest_loader = DataLoader(dataset_test, batch_size=16)\nval_loader = DataLoader(dataset_val, batch_size=16)\npred_loader = DataLoader(dataset_pred, batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.865750Z","iopub.execute_input":"2023-10-30T12:16:55.865983Z","iopub.status.idle":"2023-10-30T12:16:55.885302Z","shell.execute_reply.started":"2023-10-30T12:16:55.865963Z","shell.execute_reply":"2023-10-30T12:16:55.884536Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"for i in train_loader:\n    print(i)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.886415Z","iopub.execute_input":"2023-10-30T12:16:55.886745Z","iopub.status.idle":"2023-10-30T12:16:55.945123Z","shell.execute_reply.started":"2023-10-30T12:16:55.886713Z","shell.execute_reply":"2023-10-30T12:16:55.944218Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[tensor([[    0,   104, 27633,  ...,     1,     1,     1],\n        [    0, 15724,    18,  ...,     1,     1,     1],\n        [    0,   448, 32232,  ...,     1,     1,     1],\n        [    0,  1039,   846,  ...,     1,     1,     1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), tensor([1, 0, 0, 0])]\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, RobertaForSequenceClassification, AlbertForSequenceClassification, XLNetForSequenceClassification, AutoModelForSequenceClassification, AutoModel","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.950408Z","iopub.execute_input":"2023-10-30T12:16:55.950693Z","iopub.status.idle":"2023-10-30T12:16:55.955427Z","shell.execute_reply.started":"2023-10-30T12:16:55.950669Z","shell.execute_reply":"2023-10-30T12:16:55.954454Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# model_seq = RobertaForSequenceClassification.from_pretrained(\n#                                       'roberta-base', \n#                                       num_labels = 2,\n#                                       output_attentions = False,\n#                                       output_hidden_states = True,\n#                                       ignore_mismatched_sizes=True\n#                                      )","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.956700Z","iopub.execute_input":"2023-10-30T12:16:55.956963Z","iopub.status.idle":"2023-10-30T12:16:55.966890Z","shell.execute_reply.started":"2023-10-30T12:16:55.956941Z","shell.execute_reply":"2023-10-30T12:16:55.966048Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Concat Pooling\nclass Model(torch.nn.Module):\n    \n    def __init__(self, ):\n        \n        super(Model, self).__init__()\n        self.base_model = RobertaForSequenceClassification.from_pretrained(\n                                      'roberta-base', \n                                      num_labels = 2,\n                                      output_attentions = False,\n                                      output_hidden_states = True,\n                                      ignore_mismatched_sizes=True\n                                     )\n        self.fc1 = torch.nn.Linear(3072, 768)\n        self.fc2 = torch.nn.Linear(768,1)\n        \n    def forward(self, ids, masks):\n        x = self.base_model(ids, attention_mask=masks)\n        all_hidden_states = torch.stack(x['hidden_states'])\n\n        concatenate_pooling = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        x = concatenate_pooling[:, 0]\n\n#         logits = nn.Linear(config.hidden_size*4, 1)(concatenate_pooling)\n#         x = torch.cat((x[-1], x[-2], x[-3], x[-4]),-1)\n#         x = x[:, 0]\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.967994Z","iopub.execute_input":"2023-10-30T12:16:55.968306Z","iopub.status.idle":"2023-10-30T12:16:55.980171Z","shell.execute_reply.started":"2023-10-30T12:16:55.968282Z","shell.execute_reply":"2023-10-30T12:16:55.979390Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# ## Mean-MaxPooling\n# class Model(torch.nn.Module):\n    \n#     def __init__(self, ):\n        \n#         super(Model, self).__init__()\n#         self.base_model = AutoModel.from_pretrained(\n#                                       'roberta-base', \n#                                       num_labels = 2,\n#                                       output_attentions = False,\n#                                       output_hidden_states = True,\n#                                       ignore_mismatched_sizes=True,\n#                                      )\n#         self.fc1 = torch.nn.Linear(768, 384)\n#         self.fc2 = torch.nn.Linear(384,1)\n        \n#     def forward(self, ids, masks):\n#         x = self.base_model(ids, attention_mask=masks)[0]\n# #         mean_pooling_embeddings = torch.mean(x, 1)\n# #         _, max_pooling_embeddings = torch.max(x, 1)\n# #         x = torch.cat((mean_pooling_embeddings, max_pooling_embeddings),1)\n#         x = x[:, 0]\n#         x = self.fc1(x)\n#         x = self.fc2(x)\n#         return x","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.981287Z","iopub.execute_input":"2023-10-30T12:16:55.981660Z","iopub.status.idle":"2023-10-30T12:16:55.995773Z","shell.execute_reply.started":"2023-10-30T12:16:55.981586Z","shell.execute_reply":"2023-10-30T12:16:55.994891Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# # Attention Pooling\n# class AttentionPooling(nn.Module):\n#     def __init__(self, num_layers , hidden_size, hiddendim_fc):\n#         super(AttentionPooling, self).__init__()\n#         self.num_hidden_layers = num_layers\n#         self.hidden_size = hidden_size\n#         self.hiddendim_fc = hiddendim_fc\n#         self.dropout = nn.Dropout(0.1)\n\n#         q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size))\n#         self.q = nn.Parameter(torch.from_numpy(q_t)).float()\n#         self.q = (self.q).to(device)\n#         w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc))\n#         self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float()\n#         self.w_h = (self.w_h).to(device)\n\n#     def forward(self, all_hidden_states):\n#         hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n#                                      for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n#         hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n#         out = self.attention(hidden_states)\n#         out = self.dropout(out)\n#         return out\n\n#     def attention(self, h):\n#         h = h.to(device)\n#         v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n#         v = F.softmax(v, -1)\n#         v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n#         v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n#         return v\n    \n# class Model(torch.nn.Module):\n    \n#     def __init__(self, ):\n#         super(Model, self).__init__()\n#         self.base_model = AutoModel.from_pretrained(\n#                                       'roberta-base', \n#                                       num_labels = 2,\n#                                       output_attentions = False,\n#                                       output_hidden_states = True,\n#                                       ignore_mismatched_sizes=True\n#                                      )\n#         self.pooler = AttentionPooling(12, 768, 1536)\n#         self.fc1 = torch.nn.Linear(1536, 768)\n#         self.fc2 = torch.nn.Linear(768,1)\n        \n#     def forward(self, ids, masks):\n#         x = self.base_model(ids, attention_mask=masks)['hidden_states']\n#         x = torch.stack(x)\n#         x = self.pooler(x)\n# #         x = x[:, 0]\n#         x = self.fc1(x)\n#         x = self.fc2(x)\n#         return x","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:55.997188Z","iopub.execute_input":"2023-10-30T12:16:55.997649Z","iopub.status.idle":"2023-10-30T12:16:56.012132Z","shell.execute_reply.started":"2023-10-30T12:16:55.997617Z","shell.execute_reply":"2023-10-30T12:16:56.011028Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# ## WeightedPooling\n# class WeightedLayerPooling(nn.Module):\n#     def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n#         super(WeightedLayerPooling, self).__init__()\n#         self.layer_start = layer_start\n#         self.num_hidden_layers = num_hidden_layers\n#         self.layer_weights = layer_weights if layer_weights is not None \\\n#             else nn.Parameter(\n#                 torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n#             )\n#     def forward(self, all_hidden_states):\n#         all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n#         weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n#         weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n#         return weighted_average\n# class Model(torch.nn.Module):\n    \n#     def __init__(self, ):\n#         super(Model, self).__init__()\n#         self.base_model = AutoModel.from_pretrained(\n#                                       'roberta-base', \n#                                       num_labels = 2,\n#                                       output_attentions = False,\n#                                       output_hidden_states = True,\n#                                       ignore_mismatched_sizes=True\n#                                      )\n        \n#         self.pooler = WeightedLayerPooling(12, layer_start=9, layer_weights=None)\n#         self.fc1 = torch.nn.Linear(768, 384)\n#         self.fc2 = torch.nn.Linear(384,1)\n        \n#     def forward(self, ids, masks):\n#         x = self.base_model(ids, attention_mask=masks)['hidden_states']\n#         x = torch.stack(x)\n#         x = self.pooler(x)\n#         x = x[:, 0]\n#         x = self.fc1(x)\n#         x = self.fc2(x)\n#         return x","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-30T12:16:56.013557Z","iopub.execute_input":"2023-10-30T12:16:56.013868Z","iopub.status.idle":"2023-10-30T12:16:56.027841Z","shell.execute_reply.started":"2023-10-30T12:16:56.013844Z","shell.execute_reply":"2023-10-30T12:16:56.026963Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# # ## LSTM Pooling\n\n# class LSTMPooling(nn.Module):\n#     def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n#         super(LSTMPooling, self).__init__()\n#         self.num_hidden_layers = num_layers\n#         self.hidden_size = hidden_size\n#         self.hiddendim_lstm = hiddendim_lstm\n#         self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n#         self.dropout = nn.Dropout(0.1)\n    \n#     def forward(self, all_hidden_states):\n#         ## forward\n#         hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n#                                      for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n#         hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n#         out, _ = self.lstm(hidden_states, None)\n#         out = self.dropout(out[:, -1, :])\n#         return out\n# class Model(torch.nn.Module):\n#     def __init__(self, ):\n#         super(Model, self).__init__()\n#         self.base_model = AutoModel.from_pretrained(\n#                                       'roberta-base', \n#                                       num_labels = 2,\n#                                       output_attentions = False,\n#                                       output_hidden_states = True,\n#                                       ignore_mismatched_sizes=True\n#                                      )\n#         self.pooler = LSTMPooling(12, 768, 256)\n#         self.fc1 = torch.nn.Linear(256, 128)\n#         self.fc2 = torch.nn.Linear(128,1)\n#     def forward(self, ids, masks):\n#         x = self.base_model(ids, attention_mask=masks)['hidden_states']\n#         x = torch.stack(x)\n#         x = self.pooler(x)\n# #         x = x[:, 0]\n#         x = self.fc1(x)\n#         x = self.fc2(x)\n#         return x","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:56.029093Z","iopub.execute_input":"2023-10-30T12:16:56.029631Z","iopub.status.idle":"2023-10-30T12:16:56.043866Z","shell.execute_reply.started":"2023-10-30T12:16:56.029601Z","shell.execute_reply":"2023-10-30T12:16:56.043017Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model_seq = Model()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:16:56.044948Z","iopub.execute_input":"2023-10-30T12:16:56.045298Z","iopub.status.idle":"2023-10-30T12:16:59.636880Z","shell.execute_reply.started":"2023-10-30T12:16:56.045266Z","shell.execute_reply":"2023-10-30T12:16:59.636118Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb3a79813f64c5dbc0068978808cf02"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_seq.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-30T12:16:59.637951Z","iopub.execute_input":"2023-10-30T12:16:59.638231Z","iopub.status.idle":"2023-10-30T12:17:04.865033Z","shell.execute_reply.started":"2023-10-30T12:16:59.638206Z","shell.execute_reply":"2023-10-30T12:17:04.864096Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Model(\n  (base_model): RobertaForSequenceClassification(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): RobertaEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (classifier): RobertaClassificationHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n    )\n  )\n  (fc1): Linear(in_features=3072, out_features=768, bias=True)\n  (fc2): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"epoch = 3\noptimizer = AdamW(model_seq.parameters(), lr=1e-5,eps = 1e-8)\n# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = epoch)\nlr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = len(train_loader)*epoch)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:17:04.866872Z","iopub.execute_input":"2023-10-30T12:17:04.867631Z","iopub.status.idle":"2023-10-30T12:17:04.876500Z","shell.execute_reply.started":"2023-10-30T12:17:04.867585Z","shell.execute_reply":"2023-10-30T12:17:04.875521Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# def f1_score_func(preds, labels):\n#     preds_flat = preds.flatten()\n#     labels_flat = labels.flatten()\n#     return f1_score(labels_flat, preds_flat, average = 'weighted')\ncriterion = torch.nn.BCEWithLogitsLoss(reduction='mean')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:17:04.877474Z","iopub.execute_input":"2023-10-30T12:17:04.877772Z","iopub.status.idle":"2023-10-30T12:17:04.897427Z","shell.execute_reply.started":"2023-10-30T12:17:04.877747Z","shell.execute_reply":"2023-10-30T12:17:04.896503Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# def f1_score_func(preds, labels):\n#     preds_flat = np.argmax(preds, axis=1).flatten()\n#     labels_flat = labels.flatten()\n#     return f1_score(labels_flat, preds_flat, average = 'weighted')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:17:04.898407Z","iopub.execute_input":"2023-10-30T12:17:04.898680Z","iopub.status.idle":"2023-10-30T12:17:04.909522Z","shell.execute_reply.started":"2023-10-30T12:17:04.898657Z","shell.execute_reply":"2023-10-30T12:17:04.908505Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def train(num_epoch, model):\n    itr = 0\n    logits = []\n    low_loss=999\n    best_score = 0\n    threshold = 0.5\n    for epoch in tqdm(range(num_epoch)):\n        losses = 0\n#         total = 1\n        score = 0\n        model.train()\n        for data in train_loader:\n            optimizer.zero_grad()\n            batch = tuple(b.to(device) for b in data)\n            inputs = {\n                'input_ids':      batch[0],\n                'attention_mask': batch[1],\n                'labels':         batch[2],\n                 }   \n#             outputs = model(**inputs)\n#             loss = outputs[0]\n#             logits = outputs[1]\n            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n            loss = criterion(outputs,inputs['labels'][:,None].float())\n            logits = [1 if x>threshold else 0 for x in outputs]\n#             logits = (torch.tensor(outputs)).argmax(dim=1)\n            logits = torch.tensor(logits).detach().cpu().numpy()\n            losses += loss.item()\n            label_ids = inputs['labels'].cpu().numpy()\n            score += f1_score_func(logits,label_ids).item()\n            loss.backward()\n#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        train_loss = losses/len(train_loader)\n        train_score = score/len(train_loader)\n#         if lr_scheduler is not None:\n#             lr_scheduler.step()\n        print(f'''Epoch {epoch+1}: Train loss    : {train_loss}, \n         Train_score : {train_score},''')\n        evaluation(model)\n        test_score = test(model)\n        if train_loss < low_loss:\n            low_loss = train_loss\n            torch.save(model.state_dict(),'lesslossmodel.pt')\n            print(f'Lessloss model saved at {epoch+1}')\n        if best_score < test_score:\n            best_score = test_score\n            torch.save(model.state_dict(),f'Highscoremodel.pt')\n            print(f'Highscore model saved at {epoch+1}')\n        print('-'*110)\n        torch.save(model.state_dict(),f'Epoch{epoch+1}model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:17:04.910611Z","iopub.execute_input":"2023-10-30T12:17:04.910847Z","iopub.status.idle":"2023-10-30T12:17:04.923549Z","shell.execute_reply.started":"2023-10-30T12:17:04.910827Z","shell.execute_reply":"2023-10-30T12:17:04.922490Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def evaluation(model):\n    model.eval()\n    valid_score = 0\n    valid_loss = 0\n    threshold = 0.5\n\n    with torch.inference_mode():\n        for data in val_loader:\n            batch = tuple(b.to(device) for b in data)\n        \n            inputs = {\n                'input_ids':      batch[0],\n                'attention_mask': batch[1],\n                'labels':         batch[2],\n                 } \n            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n#             logits = [1 if x>threshold else 0 for x in outputs]\n#             outputs = model(**inputs)\n#             outputs = model(inputs['input_ids'], inputs['attention_mask'])\n            logits = [1 if x>threshold else 0 for x in outputs]\n#             logits = torch.tensor(logits)\n#             label_ids = torch.tensor(inputs['labels'].cpu().numpy())\n#             logits = (torch.tensor(outputs)).argmax(dim=1)\n#             logits = outputs[1]\n            logits = torch.tensor(logits).detach().cpu().numpy()\n            label_ids = inputs['labels'].cpu().numpy()\n            valid_score += f1_score_func(logits,label_ids).item()\n        valid_score /= len(val_loader)\n        print(f\"         Valid_Score : {valid_score},\")","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:17:04.924633Z","iopub.execute_input":"2023-10-30T12:17:04.924886Z","iopub.status.idle":"2023-10-30T12:17:04.938863Z","shell.execute_reply.started":"2023-10-30T12:17:04.924864Z","shell.execute_reply":"2023-10-30T12:17:04.937988Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def test(model):\n    model.eval()\n    test_score = 0\n    threshold = 0.5\n    test_loss = 0\n    with torch.inference_mode():\n        for data in test_loader:\n            batch = tuple(b.to(device) for b in data)\n        \n            inputs = {\n                'input_ids':      batch[0],\n                'attention_mask': batch[1],\n                'labels':         batch[2],\n                 }      \n            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n#             logits = outputs\n#             logits = logits.detach().cpu().numpy()\n#             label_ids = inputs['labels'].cpu().numpy()\n#             outputs = model(**inputs)\n#             outputs = model(inputs['input_ids'], inputs['attention_mask'])\n            logits = [1 if x>threshold else 0 for x in outputs]\n#             logits = torch.tensor(logits)\n#             label_ids = torch.tensor(inputs['labels'].cpu().numpy())\n#             logits = (torch.tensor(outputs)).argmax(dim=1)\n#             logits = outputs[1]\n            logits = torch.tensor(logits).detach().cpu().numpy()\n            label_ids = inputs['labels'].cpu().numpy()\n            test_score += f1_score_func(logits,label_ids).item()\n        test_score /= len(test_loader)\n        print(f\"         Test_Score : {test_score}.\")\n        return test_score","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:17:04.939966Z","iopub.execute_input":"2023-10-30T12:17:04.940304Z","iopub.status.idle":"2023-10-30T12:17:04.954950Z","shell.execute_reply.started":"2023-10-30T12:17:04.940273Z","shell.execute_reply":"2023-10-30T12:17:04.954101Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# tokenizer.tokenize(train_csv.loc[0]['text'])","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:17:04.956315Z","iopub.execute_input":"2023-10-30T12:17:04.956703Z","iopub.status.idle":"2023-10-30T12:17:04.970147Z","shell.execute_reply.started":"2023-10-30T12:17:04.956671Z","shell.execute_reply":"2023-10-30T12:17:04.969337Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"%%time\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\ntrain(epoch,model_seq) ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-30T12:17:04.971279Z","iopub.execute_input":"2023-10-30T12:17:04.971865Z","iopub.status.idle":"2023-10-30T12:27:11.280014Z","shell.execute_reply.started":"2023-10-30T12:17:04.971833Z","shell.execute_reply":"2023-10-30T12:27:11.278921Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train loss    : 0.4442305914930466, \n         Train_score : 0.7788418847512747,\n         Valid_Score : 0.8404053713007077,\n         Test_Score : 0.857025222428256.\nLessloss model saved at 1\nHighscore model saved at 1\n--------------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 1/3 [03:24<06:48, 204.02s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train loss    : 0.3512654388072709, \n         Train_score : 0.8420786042585124,\n         Valid_Score : 0.8516256480504243,\n         Test_Score : 0.8520180382584124.\nLessloss model saved at 2\n--------------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 2/3 [06:45<03:22, 202.55s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train loss    : 0.2693842668943162, \n         Train_score : 0.8799143294875402,\n         Valid_Score : 0.8412049282706929,\n         Test_Score : 0.8431791870807656.\nLessloss model saved at 3\n--------------------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [10:06<00:00, 202.10s/it]","output_type":"stream"},{"name":"stdout","text":"CPU times: user 8min 17s, sys: 1min 46s, total: 10min 4s\nWall time: 10min 6s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# pred_model = RobertaForSequenceClassification.from_pretrained(\n#                                       'roberta-base', \n#                                       num_labels = 2,\n#                                       output_attentions = False,\n#                                       output_hidden_states = True,\n#                                       ignore_mismatched_sizes=True\n#                                      )\n# pred_model.load_state_dict(torch.load('/kaggle/working/Highscoremodel.pt', map_location=torch.device('cuda')))\n# # pred_model.load_state_dict(torch.load('/kaggle/working/Epoch1model.pt', map_location=torch.device('cuda')))","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:53:09.224312Z","iopub.execute_input":"2023-10-30T11:53:09.224951Z","iopub.status.idle":"2023-10-30T11:53:09.229447Z","shell.execute_reply.started":"2023-10-30T11:53:09.224917Z","shell.execute_reply":"2023-10-30T11:53:09.228485Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# pred_model.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-30T11:53:09.230863Z","iopub.execute_input":"2023-10-30T11:53:09.231477Z","iopub.status.idle":"2023-10-30T11:53:09.245711Z","shell.execute_reply.started":"2023-10-30T11:53:09.231445Z","shell.execute_reply":"2023-10-30T11:53:09.244687Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# def perform_inference(text):\n#     tokens = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors=\"pt\")\n\n#     with torch.no_grad():\n#         outputs = pred_model(**tokens.to(device))\n\n#     logits = outputs[0]\n#     probabilities = torch.softmax(logits, dim=1)\n#     predicted_class = torch.argmax(probabilities, dim=1).item()\n#     return predicted_class\n\n# test_df['target'] = test_df['text'].apply(perform_inference)\n\n# submission = test_df[['id','target']]\n\n# submission.to_csv('custompretrain1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:53:09.246767Z","iopub.execute_input":"2023-10-30T11:53:09.247084Z","iopub.status.idle":"2023-10-30T11:53:09.257756Z","shell.execute_reply.started":"2023-10-30T11:53:09.247052Z","shell.execute_reply":"2023-10-30T11:53:09.257038Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# test_df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:53:09.259256Z","iopub.execute_input":"2023-10-30T11:53:09.259567Z","iopub.status.idle":"2023-10-30T11:53:09.268530Z","shell.execute_reply.started":"2023-10-30T11:53:09.259537Z","shell.execute_reply":"2023-10-30T11:53:09.267673Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# test_df","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:53:09.269509Z","iopub.execute_input":"2023-10-30T11:53:09.269741Z","iopub.status.idle":"2023-10-30T11:53:09.279382Z","shell.execute_reply.started":"2023-10-30T11:53:09.269720Z","shell.execute_reply":"2023-10-30T11:53:09.278615Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"pred_model = Model()\npred_model.load_state_dict(torch.load('/kaggle/working/Highscoremodel.pt', map_location=torch.device('cuda')))\n# pred_model.load_state_dict(torch.load('/kaggle/working/Epoch1model.pt', map_location=torch.device('cuda')))\npred_model.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-30T12:28:07.866774Z","iopub.execute_input":"2023-10-30T12:28:07.867558Z","iopub.status.idle":"2023-10-30T12:28:09.997033Z","shell.execute_reply.started":"2023-10-30T12:28:07.867527Z","shell.execute_reply":"2023-10-30T12:28:09.996115Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"Model(\n  (base_model): RobertaForSequenceClassification(\n    (roberta): RobertaModel(\n      (embeddings): RobertaEmbeddings(\n        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n        (position_embeddings): Embedding(514, 768, padding_idx=1)\n        (token_type_embeddings): Embedding(1, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): RobertaEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x RobertaLayer(\n            (attention): RobertaAttention(\n              (self): RobertaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): RobertaSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): RobertaIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): RobertaOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (classifier): RobertaClassificationHead(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n    )\n  )\n  (fc1): Linear(in_features=3072, out_features=768, bias=True)\n  (fc2): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# def pred_tokenize(data,max_len=512) :\n#     encoded = tokenizer.batch_encode_plus(\n#                                             list(data['text'].values),\n#                                             add_special_tokens=True,\n#                                             return_attention_mask=True,\n#                                             pad_to_max_length=True,\n#                                             max_length=256,\n#                                             return_tensors='pt'\n#                                         )\n#     return encoded['input_ids'], encoded['attention_mask']\n# input_ids_pred, attention_masks_pred = pred_tokenize(test_df)\n# dataset_pred = TensorDataset(input_ids_pred, attention_masks_pred)#, sampler = RandomSampler(val_dataset))\n# pred_loader = DataLoader(dataset_pred, batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:53:11.316548Z","iopub.execute_input":"2023-10-30T11:53:11.316901Z","iopub.status.idle":"2023-10-30T11:53:11.321841Z","shell.execute_reply.started":"2023-10-30T11:53:11.316869Z","shell.execute_reply":"2023-10-30T11:53:11.320981Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\npred_model.eval()\n\nscore = 0\npredictions = []\nall_labels = []\n\nfor batch in pred_loader:\n\n    batch = tuple(b.to(device) for b in batch)\n    tokens = batch[0]\n    masks = batch[1]\n\n    with torch.no_grad():        \n        outputs = pred_model(tokens, masks)\n    threshold = 0.8\n\n    batch_predictions = [1 if output.item() > threshold else 0 for output in outputs]\n    predictions.extend(batch_predictions)\nprint(Counter(predictions))","metadata":{"papermill":{"duration":0.196383,"end_time":"2023-10-06T15:28:06.138183","exception":false,"start_time":"2023-10-06T15:28:05.941800","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-10-30T12:29:26.911119Z","iopub.execute_input":"2023-10-30T12:29:26.911863Z","iopub.status.idle":"2023-10-30T12:29:55.052084Z","shell.execute_reply.started":"2023-10-30T12:29:26.911829Z","shell.execute_reply":"2023-10-30T12:29:55.051101Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Counter({0: 2023, 1: 1240})\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission['target']=predictions\nsubmission['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:30:07.813509Z","iopub.execute_input":"2023-10-30T12:30:07.814539Z","iopub.status.idle":"2023-10-30T12:30:07.834976Z","shell.execute_reply.started":"2023-10-30T12:30:07.814504Z","shell.execute_reply":"2023-10-30T12:30:07.834123Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"target\n0    2023\n1    1240\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv('Concat-rep.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T12:30:19.184948Z","iopub.execute_input":"2023-10-30T12:30:19.185342Z","iopub.status.idle":"2023-10-30T12:30:19.200333Z","shell.execute_reply.started":"2023-10-30T12:30:19.185312Z","shell.execute_reply":"2023-10-30T12:30:19.199402Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}